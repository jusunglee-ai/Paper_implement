{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-EjvsPg9uMk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.init\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_EPOCHS = 90\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.9\n",
        "LR_DECAY = 0.0005\n",
        "LR_INIT = 0.01\n",
        "IMAGE_DIM = 227 # pixels\n",
        "NUM_CLASSES = 10\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "  def __init__(self,in_channel,out_channel_1,out_channel_2,out_channel_3,out_channel_4,**kwargs):\n",
        "    #Four output channel for each parallel block of network\n",
        "    super(Inception,self).__init__()\n",
        "\n",
        "    self.p1_1=nn.Conv2d(in_channel,out_channel_1,kernel_size=1)#1x1Conv\n",
        "\n",
        "    self.p2_1=nn.Conv2d(in_channel,out_channel_2[0],kernel_size=1)#1x1Conv\n",
        "    self.p2_2=nn.Conv2d(out_channel_2[0],out_channel_2[1],kernel_size=3,padding=1)#3x3Conv\n",
        "\n",
        "    self.p3_1=nn.Conv2d(in_channel,out_channel_3[0],kernel_size=1)#1x1Conv\n",
        "    self.p3_2=nn.Conv2d(out_channel_3[0],out_channel_3[1],kernel_size=5,padding=2)#5x5Conv\n",
        "\n",
        "    self.p4_1=nn.MaxPool2d(kernel_size=3,stride=1,padding=1)#3x3 MaxPool\n",
        "    self.p4_2=nn.Conv2d(in_channel,out_channel_4,kernel_size=1)#1x1 Conv\n",
        "\n",
        "  def forward(self,x):\n",
        "    p1=F.relu(self.p1_1(x))\n",
        "    p2=F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
        "    p3=F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
        "    p4=F.relu(self.p4_2(self.p4_1(x)))\n",
        "\n",
        "\n",
        "    return torch.cat((p1,p2,p3,p4),dim=1)\n",
        "    #Finally, the outputs along each path are concatenated\n",
        "    #along the channel dimension and comprise the block's output.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BevC7W6s_VxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,**kwargs):\n",
        "      super(ConvBlock,self).__init__()\n",
        "      self.conv=nn.Conv2d(in_channels,out_channels,**kwargs)\n",
        "      self.relu=nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "      x=self.conv(x)\n",
        "      x=self.relu(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "F1ZUc1ttZUIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionAux(nn.Module):\n",
        "  def __init__(self,in_channels,num_classes):\n",
        "    super(InceptionAux,self).__init__()\n",
        "    self.avgpool=nn.AvgPool2d(kernel_size=5,stride=3)\n",
        "    self.conv=ConvBlock(in_channels,128,kernel_size=1,stride=1,padding=0)\n",
        "    self.fc=nn.Sequential(\n",
        "        nn.Linear(4*4*128,1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout2d(p=0.7),\n",
        "        nn.Linear(1024,num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.avgpool(x)\n",
        "    x=self.conv(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    return self.fc(x)"
      ],
      "metadata": {
        "id": "bBnJBtcvZUTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogleNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(GoogleNet, self).__init__()\n",
        "\n",
        "        self.training = True\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.LocalResponseNorm(2)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            ConvBlock(64, 64, kernel_size=1),\n",
        "            ConvBlock(64, 192, kernel_size=3, padding=1),\n",
        "            nn.LocalResponseNorm(2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.inception_3a = Inception(192, 64, (96, 128), (16, 32), 32)\n",
        "        self.inception_3b = Inception(256, 128, (128, 192), (32, 96), 64)\n",
        "        self.maxpool_3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception_4a = Inception(480, 192, (96, 208), (16, 48), 64)\n",
        "        self.aux1 = InceptionAux(512, num_classes)\n",
        "\n",
        "        self.inception_4b = Inception(512, 160, (112, 224), (24, 64), 64)\n",
        "        self.inception_4c = Inception(512, 128, (128, 256), (24, 64), 64)\n",
        "        self.inception_4d = Inception(512, 112, (144, 288), (32, 64), 64)\n",
        "        self.aux2 = InceptionAux(528, num_classes)\n",
        "\n",
        "        self.inception_4e = Inception(528, 256, (160, 320), (32, 128), 128)\n",
        "        self.maxpool_4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception_5a = Inception(832, 256, (160, 320), (32, 128), 128)\n",
        "        self.inception_5b = Inception(832, 384, (192, 384), (48, 128), 128)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dropout = nn.Dropout2d(p=0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.inception_3a(x)\n",
        "        x = self.inception_3b(x)\n",
        "        x = self.maxpool_3(x)\n",
        "\n",
        "        x = self.inception_4a(x)\n",
        "        if self.training:\n",
        "            out1 = self.aux1(x)\n",
        "\n",
        "        x = self.inception_4b(x)\n",
        "        x = self.inception_4c(x)\n",
        "        x = self.inception_4d(x)\n",
        "        if self.training:\n",
        "            out2 = self.aux2(x)\n",
        "\n",
        "        x = self.inception_4e(x)\n",
        "        x = self.maxpool_4(x)\n",
        "\n",
        "        x = self.inception_5a(x)\n",
        "        x = self.inception_5b(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        if self.training:\n",
        "            return [x, out1, out2]\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def set_train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def set_eval(self):\n",
        "        self.training = False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oacam0ygXx4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([#Compose는 Transform할 리스트를 구성함\n",
        "    #227x227로 하는 이유는 AlexNet의 Input이 227x227이기 때문에 데이터를 Resize해서 설정해줌 왜냐하면 MNIST데이터는 28x28크기의 데이터셋이기 때문임\n",
        "    transforms.Resize(227),\n",
        "    transforms.ToTensor()#ToTensor: PIL image 혹은 numpy.ndarray를 Tensor로 바꿈\n",
        "])\n",
        "mnist_train=datasets.MNIST(root='MNIST_data/',#MNIST 데이터 다운로드 경로 설정\n",
        "                        train=True,#훈련용 데이터로 다운받을건지에 대한 여부 True시 훈련 데이터로 다운로드 받음\n",
        "                        download=True,\n",
        "                        transform=transform)\n",
        "mnist_test=datasets.MNIST(root='MNIST_data/',#MNIST 데이터 다운로드 경로 설정\n",
        "                       train=False,#훈련용 데이터로 다운 받을건지에 대한 여부 FALSE로 설정하였음으로 훈련용 데이터가 아닌 테스트용 데이터로 받음\n",
        "                       download=True,\n",
        "                       transform=transform)\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(dataset=mnist_train,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=mnist_test,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)"
      ],
      "metadata": {
        "id": "x4eC8hqziWzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GoogleNet(1,10).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=MOMENTUM)"
      ],
      "metadata": {
        "id": "B5fzNY3djTQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary as summary_\n",
        "\n",
        "summary_(model,(1,224,224),BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZAEB--MlZ2n",
        "outputId": "49fb516a-cfe4-44ce-b442-ec679affbfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [128, 64, 112, 112]           3,200\n",
            "              ReLU-2        [128, 64, 112, 112]               0\n",
            "         ConvBlock-3        [128, 64, 112, 112]               0\n",
            "         MaxPool2d-4          [128, 64, 56, 56]               0\n",
            " LocalResponseNorm-5          [128, 64, 56, 56]               0\n",
            "            Conv2d-6          [128, 64, 56, 56]           4,160\n",
            "              ReLU-7          [128, 64, 56, 56]               0\n",
            "         ConvBlock-8          [128, 64, 56, 56]               0\n",
            "            Conv2d-9         [128, 192, 56, 56]         110,784\n",
            "             ReLU-10         [128, 192, 56, 56]               0\n",
            "        ConvBlock-11         [128, 192, 56, 56]               0\n",
            "LocalResponseNorm-12         [128, 192, 56, 56]               0\n",
            "        MaxPool2d-13         [128, 192, 28, 28]               0\n",
            "           Conv2d-14          [128, 64, 28, 28]          12,352\n",
            "           Conv2d-15          [128, 96, 28, 28]          18,528\n",
            "           Conv2d-16         [128, 128, 28, 28]         110,720\n",
            "           Conv2d-17          [128, 16, 28, 28]           3,088\n",
            "           Conv2d-18          [128, 32, 28, 28]          12,832\n",
            "        MaxPool2d-19         [128, 192, 28, 28]               0\n",
            "           Conv2d-20          [128, 32, 28, 28]           6,176\n",
            "        Inception-21         [128, 256, 28, 28]               0\n",
            "           Conv2d-22         [128, 128, 28, 28]          32,896\n",
            "           Conv2d-23         [128, 128, 28, 28]          32,896\n",
            "           Conv2d-24         [128, 192, 28, 28]         221,376\n",
            "           Conv2d-25          [128, 32, 28, 28]           8,224\n",
            "           Conv2d-26          [128, 96, 28, 28]          76,896\n",
            "        MaxPool2d-27         [128, 256, 28, 28]               0\n",
            "           Conv2d-28          [128, 64, 28, 28]          16,448\n",
            "        Inception-29         [128, 480, 28, 28]               0\n",
            "        MaxPool2d-30         [128, 480, 14, 14]               0\n",
            "           Conv2d-31         [128, 192, 14, 14]          92,352\n",
            "           Conv2d-32          [128, 96, 14, 14]          46,176\n",
            "           Conv2d-33         [128, 208, 14, 14]         179,920\n",
            "           Conv2d-34          [128, 16, 14, 14]           7,696\n",
            "           Conv2d-35          [128, 48, 14, 14]          19,248\n",
            "        MaxPool2d-36         [128, 480, 14, 14]               0\n",
            "           Conv2d-37          [128, 64, 14, 14]          30,784\n",
            "        Inception-38         [128, 512, 14, 14]               0\n",
            "        AvgPool2d-39           [128, 512, 4, 4]               0\n",
            "           Conv2d-40           [128, 128, 4, 4]          65,664\n",
            "             ReLU-41           [128, 128, 4, 4]               0\n",
            "        ConvBlock-42           [128, 128, 4, 4]               0\n",
            "           Linear-43                [128, 1024]       2,098,176\n",
            "             ReLU-44                [128, 1024]               0\n",
            "        Dropout2d-45                [128, 1024]               0\n",
            "           Linear-46                  [128, 10]          10,250\n",
            "     InceptionAux-47                  [128, 10]               0\n",
            "           Conv2d-48         [128, 160, 14, 14]          82,080\n",
            "           Conv2d-49         [128, 112, 14, 14]          57,456\n",
            "           Conv2d-50         [128, 224, 14, 14]         226,016\n",
            "           Conv2d-51          [128, 24, 14, 14]          12,312\n",
            "           Conv2d-52          [128, 64, 14, 14]          38,464\n",
            "        MaxPool2d-53         [128, 512, 14, 14]               0\n",
            "           Conv2d-54          [128, 64, 14, 14]          32,832\n",
            "        Inception-55         [128, 512, 14, 14]               0\n",
            "           Conv2d-56         [128, 128, 14, 14]          65,664\n",
            "           Conv2d-57         [128, 128, 14, 14]          65,664\n",
            "           Conv2d-58         [128, 256, 14, 14]         295,168\n",
            "           Conv2d-59          [128, 24, 14, 14]          12,312\n",
            "           Conv2d-60          [128, 64, 14, 14]          38,464\n",
            "        MaxPool2d-61         [128, 512, 14, 14]               0\n",
            "           Conv2d-62          [128, 64, 14, 14]          32,832\n",
            "        Inception-63         [128, 512, 14, 14]               0\n",
            "           Conv2d-64         [128, 112, 14, 14]          57,456\n",
            "           Conv2d-65         [128, 144, 14, 14]          73,872\n",
            "           Conv2d-66         [128, 288, 14, 14]         373,536\n",
            "           Conv2d-67          [128, 32, 14, 14]          16,416\n",
            "           Conv2d-68          [128, 64, 14, 14]          51,264\n",
            "        MaxPool2d-69         [128, 512, 14, 14]               0\n",
            "           Conv2d-70          [128, 64, 14, 14]          32,832\n",
            "        Inception-71         [128, 528, 14, 14]               0\n",
            "        AvgPool2d-72           [128, 528, 4, 4]               0\n",
            "           Conv2d-73           [128, 128, 4, 4]          67,712\n",
            "             ReLU-74           [128, 128, 4, 4]               0\n",
            "        ConvBlock-75           [128, 128, 4, 4]               0\n",
            "           Linear-76                [128, 1024]       2,098,176\n",
            "             ReLU-77                [128, 1024]               0\n",
            "        Dropout2d-78                [128, 1024]               0\n",
            "           Linear-79                  [128, 10]          10,250\n",
            "     InceptionAux-80                  [128, 10]               0\n",
            "           Conv2d-81         [128, 256, 14, 14]         135,424\n",
            "           Conv2d-82         [128, 160, 14, 14]          84,640\n",
            "           Conv2d-83         [128, 320, 14, 14]         461,120\n",
            "           Conv2d-84          [128, 32, 14, 14]          16,928\n",
            "           Conv2d-85         [128, 128, 14, 14]         102,528\n",
            "        MaxPool2d-86         [128, 528, 14, 14]               0\n",
            "           Conv2d-87         [128, 128, 14, 14]          67,712\n",
            "        Inception-88         [128, 832, 14, 14]               0\n",
            "        MaxPool2d-89           [128, 832, 7, 7]               0\n",
            "           Conv2d-90           [128, 256, 7, 7]         213,248\n",
            "           Conv2d-91           [128, 160, 7, 7]         133,280\n",
            "           Conv2d-92           [128, 320, 7, 7]         461,120\n",
            "           Conv2d-93            [128, 32, 7, 7]          26,656\n",
            "           Conv2d-94           [128, 128, 7, 7]         102,528\n",
            "        MaxPool2d-95           [128, 832, 7, 7]               0\n",
            "           Conv2d-96           [128, 128, 7, 7]         106,624\n",
            "        Inception-97           [128, 832, 7, 7]               0\n",
            "           Conv2d-98           [128, 384, 7, 7]         319,872\n",
            "           Conv2d-99           [128, 192, 7, 7]         159,936\n",
            "          Conv2d-100           [128, 384, 7, 7]         663,936\n",
            "          Conv2d-101            [128, 48, 7, 7]          39,984\n",
            "          Conv2d-102           [128, 128, 7, 7]         153,728\n",
            "       MaxPool2d-103           [128, 832, 7, 7]               0\n",
            "          Conv2d-104           [128, 128, 7, 7]         106,624\n",
            "       Inception-105          [128, 1024, 7, 7]               0\n",
            "AdaptiveAvgPool2d-106          [128, 1024, 1, 1]               0\n",
            "       Dropout2d-107                [128, 1024]               0\n",
            "          Linear-108                  [128, 10]          10,250\n",
            "================================================================\n",
            "Total params: 10,327,758\n",
            "Trainable params: 10,327,758\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 24.50\n",
            "Forward/backward pass size (MB): 9697.72\n",
            "Params size (MB): 39.40\n",
            "Estimated Total Size (MB): 9761.62\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # enumberate() : 인덱스와 원소로 이루어진 튜플(tuple)을 만들어줌\n",
        "        target = target.type(torch.LongTensor)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # 항상 backpropagation 하기전에 미분(gradient)을 zero로 만들어주고 시작해야 한다.\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target) # criterion = loss_fn\n",
        "        writer.add_scalar(\"Loss/GoogleNet train\",loss,epoch)\n",
        "        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves\n",
        "        optimizer.step() # step() : 파라미터를 업데이트함\n",
        "        if (batch_idx + 1) % 30 == 0:\n",
        "            print(\"Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "S8uPv8HjnV_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target).item()\n",
        "            writer.add_scalar(\"Loss/GoogleNet test\",test_loss/len(test_loader),epoch)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)  # -> mean\n",
        "        print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "        print('='*50)\n"
      ],
      "metadata": {
        "id": "PC8qxbeVnXmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=15\n",
        "for epoch in range(1,epochs+1):\n",
        "  train(model,device,train_loader,optimizer,epoch)\n",
        "\n",
        "\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Ug3gIHv0nYPZ",
        "outputId": "a45afd29-cae0-45e9-db2b-698dd554fd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1fed5f0aea45>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    }
  ]
}